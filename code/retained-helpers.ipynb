{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors used for plotting the posterior predictives\n",
    "COLORS = {\n",
    "    \"true\": \"tab:orange\",\n",
    "    \"predicted\": \"tab:blue\",\n",
    "    \"calibrated\": \"tab:pink\",\n",
    "    \"observations\": \"lightgrey\",\n",
    "}\n",
    "# Transparency for the posterior predictives\n",
    "FILL_ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(func, points, seed=0):\n",
    "    \"\"\"Generate a dataframe containing the covariate X, and observations Y\n",
    "\n",
    "    The X's are generated uniformly over each of the supplied segments.\n",
    "\n",
    "    Args:\n",
    "        func: a scipy.stats function\n",
    "        points: a list of dictionaries describing the points\n",
    "            The expected format: [{\"n_points\": 10, \"xlim\": [-1, 1]}, ...]\n",
    "        seed: random seed (default: {0})\n",
    "\n",
    "    Returns:\n",
    "        a pandas DataFrame with the generated X and Y\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    data = []\n",
    "    for segment in points:\n",
    "        x = np.linspace(*segment[\"xlim\"], num=segment[\"n_points\"])\n",
    "        distribution = func(x)\n",
    "        # Generate observations\n",
    "        y = distribution.rvs()\n",
    "        df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "        data.append(df)\n",
    "\n",
    "    return pd.concat(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain(func, df, frac, seed=None):\n",
    "    \"\"\"Retain a fraction of the original data corresponding to the lowest\n",
    "    posterior predictive uncertainty.\n",
    "    \n",
    "    Args:\n",
    "        func: a scipy.stats distribution of the posterior predictive\n",
    "        df: a pandas DataFrame with the data\n",
    "        frac: a fraction of the data to retain\n",
    "        seed: an optional random seed used to break the ties\n",
    "    \"\"\"\n",
    "    # Randomize the order of rows for breaking the ties\n",
    "    df = df.copy().sample(frac=1, random_state=seed)\n",
    "\n",
    "    # Retrieve the uncertainty estimates of the posterior predictive for each X\n",
    "    dist = func(df.x)\n",
    "    df[\"_uncertainty\"] = dist.std()\n",
    "\n",
    "    # Retain a portion of the observations with the lowest uncertainty\n",
    "    n = int(df.shape[0] * frac)\n",
    "    df_retained = df.nsmallest(n, \"_uncertainty\").sort_index().drop(columns=\"_uncertainty\")\n",
    "\n",
    "    return df_retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"Compute MSE\"\"\"\n",
    "    return ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Compute RMSE\"\"\"\n",
    "    return np.sqrt(mse(y_true, y_pred))\n",
    "\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Compute MAE\"\"\"\n",
    "    return (y_pred - y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakePosterior:\n",
    "    \"\"\"Fake posterior predictive centered around zero with a polynomial mean and homoscedastic aleatoric noise.\n",
    "    \n",
    "    Args:\n",
    "        x: the predictor variable to fit to\n",
    "        y: the response variable\n",
    "        degree: a polynomial degree for the fitted mean\n",
    "        gap: a tuple with the range of the gap for epistemic uncertainty\n",
    "        aleatoric: standard deviation of aleatoric noise (outside the gap)\n",
    "        epistemic: the maximum standard deviation for epistemic uncertainty (in the gap region)\n",
    "        center: the point at which epistemic uncertainty is the largest (default: 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y, degree, aleatoric, epistemic, gap, center=0):\n",
    "        self._poly = np.poly1d(np.polyfit(x, y, deg=degree))\n",
    "        low, high = gap\n",
    "        xx = np.concatenate((np.linspace(x.min(), low), [center], np.linspace(high, x.max())))\n",
    "        yy = np.concatenate((np.full(50, aleatoric), [aleatoric + epistemic], np.full(50, aleatoric)))\n",
    "        self._spline = UnivariateSpline(xx, yy, k=3, s=0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self._x = x\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self._poly(self._x)\n",
    "\n",
    "    def std(self):\n",
    "        return self._spline(self._x)\n",
    "\n",
    "    def interval(self, interval):\n",
    "        assert 0 <= interval <= 1\n",
    "        # Assuming symmetric Gaussian noise\n",
    "        q_alpha = 1 - interval\n",
    "        z = scipy.stats.norm.ppf(interval + q_alpha / 2)\n",
    "        mean = self.mean()\n",
    "        margin_error = z * self.std()\n",
    "        return mean - margin_error, mean + margin_error\n",
    "\n",
    "    def cdf(self, y):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(func, df, name, interval=0.95, observations=True, title=None, legend=True, ax=None):\n",
    "    \"\"\"Plot the distribution function and the observations\n",
    "\n",
    "    Args:\n",
    "        func: a scipy.stats distribution\n",
    "        df: a pandas DataFrame containing observations (x, y)\n",
    "        name: a description of the distribution function, e.g. \"true\" or \"predicted\"\n",
    "        interval: the width of the predictive interval (default: 0.95)\n",
    "        observations: optionally plot the observations (default: True)\n",
    "        title: an optional plot title (default: None)\n",
    "        legend: whether to show a legend (default: True)\n",
    "        ax: matplotlib axis to draw on, if any (default: None)\n",
    "    \"\"\"\n",
    "    assert 0 <= interval <= 1\n",
    "\n",
    "    x = np.linspace(df.x.min(), df.x.max(), num=1000)\n",
    "    distribution = func(x)\n",
    "    lower, upper = distribution.interval(interval)\n",
    "    point_est = distribution.mean()\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    ax.fill_between(\n",
    "        x,\n",
    "        lower,\n",
    "        upper,\n",
    "        color=COLORS[name],\n",
    "        alpha=FILL_ALPHA,\n",
    "        label=f\"{name.title()} {interval*100:.0f}% Interval\",\n",
    "    )\n",
    "    if observations:\n",
    "        ax.scatter(df.x, df.y, s=10, color=COLORS[\"observations\"], label=\"Observations\")\n",
    "    ax.plot(x, point_est, color=COLORS[name], label=f\"{name.title()} Mean\")\n",
    "    ax.set(xlabel=\"X\", ylabel=\"Y\")\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    #     if legend:\n",
    "    #         ax.legend(loc=\"upper left\")\n",
    "    if legend:\n",
    "        ax.legend(bbox_to_anchor=(1.04, 1), borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_function(*args, **kwargs):\n",
    "    plot(*args, **kwargs, name=\"true\")\n",
    "\n",
    "\n",
    "def plot_posterior_predictive(*args, **kwargs):\n",
    "    plot(*args, **kwargs, name=\"predicted\", observations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse(ppc_func, df, fractions=None, label=None, seed=0, title=None):\n",
    "    \"\"\"Visualize RMSE for the multiple fractions of retained data.\n",
    "    \n",
    "    Args:\n",
    "        ppc_func: a scipy.stats distribution for the posterior predictive\n",
    "        df: a pandas DataFrame with the predictor variable X\n",
    "        fractions: an optional list of fractions of retained data (default: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "        label: an optional legend label for the curve\n",
    "        seed: an optional random seed to break the ties when retaining data (default: 0)\n",
    "    \"\"\"\n",
    "    if fractions is None:\n",
    "        fractions = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    metrics = {}\n",
    "    for frac in fractions:\n",
    "        df_retained = retain(ppc_func, df, frac=frac, seed=seed)\n",
    "\n",
    "        y_true = df_retained.y.values\n",
    "        y_pred = ppc_func(df_retained.x).mean()\n",
    "        metrics[frac] = rmse(y_true, y_pred)\n",
    "\n",
    "    pd.Series(metrics).plot(style=\"-o\", xlim=[min(fractions) - 0.02, 1.02], label=label.title())\n",
    "    plt.xlabel(\"Fraction of Retained Data\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(title or \"RMSE vs Fraction of Test Retained Data\")\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), borderaxespad=0, title=\"Posterior Predictives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain train data, i.e. (x, y) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_interval(func, df, interval=None):\n",
    "    \"\"\"Retain a fraction of the observations from the training set\n",
    "    falling within a particular predictive interval.\n",
    "    \n",
    "    Args:\n",
    "        func: a scipy.stats distribution of the posterior predictive\n",
    "        df: a pandas DataFrame with the data\n",
    "        interval: the posterior preditive interval, between 0 and 1\n",
    "        \n",
    "    Returns:\n",
    "        df_retained: a subset of the original DataFrame, with observations\n",
    "            falling within the specified predictive interval.\n",
    "    \"\"\"\n",
    "    assert 0 <= interval <= 1\n",
    "\n",
    "    # Retrieve the predictive interval for each X\n",
    "    dist = func(df.x)\n",
    "    low, high = dist.interval(interval)\n",
    "\n",
    "    # Keep only the (x, y) observations falling within the given interval\n",
    "    mask = (df.y >= low) & (df.y <= high)\n",
    "    df_retained = df[mask]\n",
    "\n",
    "    return df_retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_frac(func, df, frac, seed=None):\n",
    "    \"\"\"Retain a fraction of observations from the training set\n",
    "    corresponding to the narrowest predictive interval.\n",
    "    \n",
    "    Args:\n",
    "        func: a scipy.stats distribution of the posterior predictive\n",
    "        df: a pandas DataFrame with the data\n",
    "        frac: a fraction of the data to retain, between 0 and 1\n",
    "        seed: an optional random seed used to break the ties\n",
    "    \"\"\"\n",
    "    assert 0 <= frac <= 1\n",
    "\n",
    "    # Randomize the order of rows for breaking the ties\n",
    "    df = df.copy().sample(frac=1, random_state=seed)\n",
    "\n",
    "    # Compute the distance of the observation (x, y) from the median\n",
    "    dist = func(df.x)\n",
    "    quantiles = dist.cdf(df.y)\n",
    "    df[\"_distance\"] = np.abs(quantiles - 0.5)\n",
    "\n",
    "    # Retain a portion of observations closest to the median\n",
    "    n = int(df.shape[0] * frac)\n",
    "    df_retained = df.nsmallest(n, \"_distance\").sort_index().drop(columns=\"_distance\")\n",
    "\n",
    "    return df_retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_interval(ppc_func, df, intervals=None, label=None, title=None, legend=True, ax=None, **kwargs):\n",
    "    \"\"\"Visualize RMSE for observations contained in different posterior predictive intervals.\n",
    "    \n",
    "    Args:\n",
    "        ppc_func: a scipy.stats distribution for the posterior predictive\n",
    "        df: a pandas DataFrame with the predictor variable X\n",
    "        intervals: an optional list of predictive intervals (default: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "        label: an optional legend label for the curve\n",
    "    \"\"\"\n",
    "    if intervals is None:\n",
    "        intervals = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    metrics = {}\n",
    "    for interval in intervals:\n",
    "        df_retained = retain_interval(ppc_func, df, interval=interval)\n",
    "\n",
    "        y_true = df_retained.y.values\n",
    "        y_pred = ppc_func(df_retained.x).mean()\n",
    "        metrics[interval] = rmse(y_true, y_pred)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    pd.Series(metrics).plot(\n",
    "        style=\"-o\", xlim=[min(intervals) - 0.02, 1.02], label=label.title(), ax=ax, **kwargs\n",
    "    )\n",
    "    ax.set(xlabel=\"Predictive Interval of Retained Data\", ylabel=\"RMSE\")\n",
    "    ax.set_title(title or \"RMSE vs Predictive Interval of Retained Training Data\")\n",
    "    if legend:\n",
    "        ax.legend(bbox_to_anchor=(1.02, 1), borderaxespad=0, title=\"Posterior Predictives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_frac(\n",
    "    ppc_func, df, fractions=None, label=None, seed=0, title=None, legend=True, ax=None, **kwargs\n",
    "):\n",
    "    \"\"\"Visualize RMSE for different fractions of retained data based on the smallest\n",
    "    posterior predictive intervals.\n",
    "    \n",
    "    Args:\n",
    "        ppc_func: a scipy.stats distribution for the posterior predictive\n",
    "        df: a pandas DataFrame with the predictor variable X\n",
    "        fractions: an optional list of fractions of retained data (default: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "        label: an optional legend label for the curve\n",
    "        seed: an optional random seed to break the ties when retaining data (default: 0)\n",
    "    \"\"\"\n",
    "    if fractions is None:\n",
    "        fractions = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    metrics = {}\n",
    "    for frac in fractions:\n",
    "        df_retained = retain_frac(ppc_func, df, frac=frac, seed=seed)\n",
    "\n",
    "        y_true = df_retained.y.values\n",
    "        y_pred = ppc_func(df_retained.x).mean()\n",
    "        metrics[frac] = rmse(y_true, y_pred)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    pd.Series(metrics).plot(\n",
    "        style=\"-o\", xlim=[min(fractions) - 0.02, 1.02], label=label.title(), ax=ax, **kwargs\n",
    "    )\n",
    "    ax.set(xlabel=\"Fraction of Retained Data\", ylabel=\"RMSE\")\n",
    "    ax.set_title(title or \"RMSE vs Fraction of Retained Training Data\")\n",
    "    if legend:\n",
    "        ax.legend(bbox_to_anchor=(1.02, 1), borderaxespad=0, title=\"Posterior Predictives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_loglik(ppc_func, df):\n",
    "    \"\"\"Compute negative log-likelihood of the observed data\n",
    "\n",
    "    Args:\n",
    "        ppc_func: a scipy.stats distribution for the posterior predictive\n",
    "        df: a pandas DataFrame with the predictor variable X\n",
    "        \n",
    "    Returns:\n",
    "        nll: negative log-likelihood of the observed data\n",
    "    \"\"\"\n",
    "    dist = ppc_func(df.x)\n",
    "    return -dist.logpdf(df.y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nll_frac(\n",
    "    ppc_func, df, fractions=None, label=None, seed=0, title=None, legend=True, ax=None, **kwargs\n",
    "):\n",
    "    \"\"\"Visualize negative log-likelihood for different fractions of retained data based on the\n",
    "    smallest posterior predictive intervals.\n",
    "    \n",
    "    Args:\n",
    "        ppc_func: a scipy.stats distribution for the posterior predictive\n",
    "        df: a pandas DataFrame with the predictor variable X\n",
    "        fractions: an optional list of fractions of retained data (default: [0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "        label: an optional legend label for the curve\n",
    "        seed: an optional random seed to break the ties when retaining data (default: 0)\n",
    "    \"\"\"\n",
    "    if fractions is None:\n",
    "        fractions = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "    metrics = {}\n",
    "    for frac in fractions:\n",
    "        df_retained = retain_frac(ppc_func, df, frac=frac, seed=seed)\n",
    "        metrics[frac] = negative_loglik(ppc_func, df_retained)\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    pd.Series(metrics).plot(\n",
    "        style=\"-o\", xlim=[min(fractions) - 0.02, 1.02], label=label.title(), ax=ax, **kwargs\n",
    "    )\n",
    "    ax.set(xlabel=\"Fraction of Retained Data\", ylabel=\"NLL\")\n",
    "    ax.set_title(title or \"NLL vs Fraction of Retained Training Data\")\n",
    "    if legend:\n",
    "        ax.legend(bbox_to_anchor=(1.02, 1), borderaxespad=0, title=\"Posterior Predictives\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
